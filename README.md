
## How to run it locally
1. Substitute HUGGINGFACEHUB_API_TOKEN in README.md. You can generate this token from your Hugging Face account.

2. Activate Python environment
  - python3 -m venv myenv
  - source myenv/bin/activate
3. Install dependencies
  - pip install -r requirements.txt  
2. Run `streamlit run ragchatbot.py`

## Idea behind Chatbot
1. We extract the text from the PDF and divide it into chunks.
2. We generate vector embeddings of the text using pre-trained sentence transformer model.
3. The system retrieves relevant information from the knowledge base based on the user's question.
4. The system builds a prompt that includes the relevant information from the knowledge base and the user's question.
5. The prompt is sent to the large language model endpoint, which generates a response based on the retrieved information.
6. Users view the response generated by the large language model endpoint.

## Future Scope
1. The accuracy of the model can be improved by using OpenAPI models.
2. The accuracy of the model can also be improved by cleaning the dataset.
3. The ingestion (extracting text from PDF and generating vector score) can be done just once instead of doing it everytime user enters question
3. The application can be scaled to upload PDF and answer questions based on the uploaded PDF.

## Evaluation Metric
1. I used rouge_score to evaluate the performance of the model 

